{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型拟合 CV部分\n",
    "采用xgboost模型 logistic regression 用于预测reorder的概率\n",
    "#重要说明：将eval_metric 从 logloss 改为 auc \n",
    "#增大了max_depth（6 -> 10） 明显提高了模型拟合分数\n",
    "#在特征和模型数据增大时 tree_method会自动切换成prob 然后会出现奇怪的错误 程序终止运行 所以这里增加了tree_method\n",
    "\n",
    "需要解决的问题 ： 目前评判标准是auc，但是kaggle上面的评分标准是mean-F1, 不知道在trainning的时候是否有必要自定义一个函数f1-score来作为eval-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Tang\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import xgboost\n",
    "from functools import partial\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train = pd.read_pickle('train.pkl')\n",
    "train.drop(['eval_set'], axis=1, inplace=True)\n",
    "train.loc[:, 'reordered'] = train.reordered.fillna(0)\n",
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"auc\"\n",
    "    ,\"eta\"              : 0.1\n",
    "    ,\"max_depth\"        : 10\n",
    "    ,\"min_child_weight\" : 5\n",
    "    ,\"gamma\"            :0.70\n",
    "    ,\"subsample\"        :1.0\n",
    "    ,\"colsample_bytree\" :0.95\n",
    "    ,\"alpha\"            :2e-05\n",
    "    ,\"lambda\"           :10\n",
    "    ,'tree_method'      :'exact'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原程序自带的记时和用于输出结果的函数（可以忽略）\n",
    "这里没有做修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path_data):\n",
    "    '''\n",
    "    --------------------------------order_product--------------------------------\n",
    "    * Unique in order_id + product_id\n",
    "    '''\n",
    "    priors = pd.read_csv(path_data + 'order_products__prior.csv', \n",
    "                     dtype={\n",
    "                            'order_id': np.int32,\n",
    "                            'product_id': np.uint16,\n",
    "                            'add_to_cart_order': np.int16,\n",
    "                            'reordered': np.int8})\n",
    "    train = pd.read_csv(path_data + 'order_products__train.csv', \n",
    "                    dtype={\n",
    "                            'order_id': np.int32,\n",
    "                            'product_id': np.uint16,\n",
    "                            'add_to_cart_order': np.int16,\n",
    "                            'reordered': np.int8})\n",
    "    '''\n",
    "    --------------------------------order--------------------------------\n",
    "    * This file tells us which set (prior, train, test) an order belongs\n",
    "    * Unique in order_id\n",
    "    * order_id in train, prior, test has no intersection\n",
    "    * this is the #order_number order of this user\n",
    "    '''\n",
    "    orders = pd.read_csv(path_data + 'orders.csv', \n",
    "                         dtype={\n",
    "                                'order_id': np.int32,\n",
    "                                'user_id': np.int64,\n",
    "                                'eval_set': 'category',\n",
    "                                'order_number': np.int16,\n",
    "                                'order_dow': np.int8,\n",
    "                                'order_hour_of_day': np.int8,\n",
    "                                'days_since_prior_order': np.float32})\n",
    "\n",
    "    #  order in prior, train, test has no duplicate\n",
    "    #  order_ids_pri = priors.order_id.unique()\n",
    "    #  order_ids_trn = train.order_id.unique()\n",
    "    #  order_ids_tst = orders[orders.eval_set == 'test']['order_id'].unique()\n",
    "    #  print(set(order_ids_pri).intersection(set(order_ids_trn)))\n",
    "    #  print(set(order_ids_pri).intersection(set(order_ids_tst)))\n",
    "    #  print(set(order_ids_trn).intersection(set(order_ids_tst)))\n",
    "\n",
    "    '''\n",
    "    --------------------------------product--------------------------------\n",
    "    * Unique in product_id\n",
    "    '''\n",
    "    products = pd.read_csv(path_data + 'products.csv')\n",
    "    aisles = pd.read_csv(path_data + \"aisles.csv\")\n",
    "    departments = pd.read_csv(path_data + \"departments.csv\")\n",
    "    sample_submission = pd.read_csv(path_data + \"sample_submission.csv\")\n",
    "    \n",
    "    return priors, train, orders, products, aisles, departments, sample_submission\n",
    "class tick_tock:\n",
    "    def __init__(self, process_name, verbose=1):\n",
    "        self.process_name = process_name\n",
    "        self.verbose = verbose\n",
    "    def __enter__(self):\n",
    "        if self.verbose:\n",
    "            print(self.process_name + \" begin ......\")\n",
    "            self.begin_time = time.time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.verbose:\n",
    "            end_time = time.time()\n",
    "            print(self.process_name + \" end ......\")\n",
    "            print('time lapsing {0} s \\n'.format(end_time - self.begin_time))\n",
    "            \n",
    "\n",
    "def ka_add_groupby_features_n_vs_1(df, group_columns_list, target_columns_list, methods_list, keep_only_stats=True, verbose=1):\n",
    "   \n",
    "    with tick_tock(\"add stats features\", verbose):\n",
    "        dicts = {\"group_columns_list\": group_columns_list , \"target_columns_list\": target_columns_list, \"methods_list\" :methods_list}\n",
    "\n",
    "        for k, v in dicts.items():\n",
    "            try:\n",
    "                if type(v) == list:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(k + \"should be a list\")\n",
    "            except TypeError as e:\n",
    "                print(e)\n",
    "                raise\n",
    "\n",
    "        grouped_name = ''.join(group_columns_list)\n",
    "        target_name = ''.join(target_columns_list)\n",
    "        combine_name = [[grouped_name] + [method_name] + [target_name] for method_name in methods_list]\n",
    "\n",
    "        df_new = df.copy()\n",
    "        grouped = df_new.groupby(group_columns_list)\n",
    "\n",
    "        the_stats = grouped[target_name].agg(methods_list).reset_index()\n",
    "        the_stats.columns = [grouped_name] + \\\n",
    "                            ['_%s_%s_by_%s' % (grouped_name, method_name, target_name) \\\n",
    "                             for (grouped_name, method_name, target_name) in combine_name]\n",
    "        if keep_only_stats:\n",
    "            return the_stats\n",
    "        else:\n",
    "            df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n",
    "        return df_new\n",
    "path_data = '../input/'\n",
    "priors, train_detail, orders, products, aisles, departments, sample_submission = load_data(path_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV模块\n",
    "\n",
    "自行创建 参考https://github.com/happycube/kaggle2017/blob/master/instacart/catboost-0723.ipynb\n",
    "#用3 fold CV 验证模型的情况 并输出f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_details = pd.merge(\n",
    "                left=train_detail,\n",
    "                 right=orders, \n",
    "                 how='left', \n",
    "                 on='order_id'\n",
    "        ).apply(partial(pd.to_numeric, errors='ignore', downcast='integer'))\n",
    "\n",
    "try:\n",
    "    df_train_gt = pd.read_csv('train.csv', index_col='order_id')\n",
    "except:\n",
    "    train_gtl = []\n",
    "\n",
    "    for uid, subset in train_details.groupby('user_id'):\n",
    "        subset1 = subset[subset.reordered == 1]\n",
    "        oid = subset.order_id.values[0]\n",
    "\n",
    "        if len(subset1) == 0:\n",
    "            train_gtl.append((oid, 'None'))\n",
    "            continue\n",
    "\n",
    "        ostr = ' '.join([str(int(e)) for e in subset1.product_id.values])\n",
    "        # .strip is needed because join can have a padding space at the end\n",
    "        train_gtl.append((oid, ostr.strip()))\n",
    "\n",
    "    df_train_gt = pd.DataFrame(train_gtl)\n",
    "\n",
    "    df_train_gt.columns = ['order_id', 'products']\n",
    "    df_train_gt.set_index('order_id', inplace=True)\n",
    "    df_train_gt.sort_index(inplace=True)\n",
    "    df_train_gt.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于处理CV的xgboost函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgboost_cv(X_train, y_train, X_val, y_val, features_to_use):\n",
    "    d_train = xgboost.DMatrix(X_train[features_to_use], y_train)\n",
    "    \n",
    "    d_val = xgboost.DMatrix(X_val[features_to_use], y_val)\n",
    "    \n",
    "    watchlist = [(d_val, \"val\"), (d_train, \"train\")];\n",
    "    \n",
    "    bst = xgboost.train(params=xgb_params, dtrain=d_train, num_boost_round = 80, evals=watchlist, verbose_eval=10)\n",
    "    \n",
    "    return bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出F1-score函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_results(df_gt, df_preds):\n",
    "    \n",
    "    df_gt_cut = df_gt.loc[df_preds.index]\n",
    "    \n",
    "    f1 = []\n",
    "    for gt, pred in zip(df_gt_cut.sort_index().products, df_preds.sort_index().products):\n",
    "        lgt = gt.replace(\"None\", \"-1\").split(' ')\n",
    "        lpred = pred.replace(\"None\", \"-1\").split(' ')\n",
    "\n",
    "        rr = (np.intersect1d(lgt, lpred))\n",
    "        precision = np.float(len(rr)) / len(lpred)\n",
    "        recall = np.float(len(rr)) / len(lgt)\n",
    "\n",
    "        denom = precision + recall\n",
    "        f1.append(((2 * precision * recall) / denom) if denom > 0 else 0)\n",
    "\n",
    "    #print(np.mean(f1))\n",
    "    return(np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV的主要部分\n",
    "3折CV 跑的时间很长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-auc:0.825454\ttrain-auc:0.826358\n",
      "[10]\tval-auc:0.830801\ttrain-auc:0.832994\n",
      "[20]\tval-auc:0.832327\ttrain-auc:0.835941\n",
      "[30]\tval-auc:0.833427\ttrain-auc:0.838581\n",
      "[40]\tval-auc:0.834325\ttrain-auc:0.841047\n",
      "[50]\tval-auc:0.834903\ttrain-auc:0.843163\n",
      "[60]\tval-auc:0.835215\ttrain-auc:0.844889\n",
      "[70]\tval-auc:0.835426\ttrain-auc:0.846238\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 0.833000183105 s \n",
      "\n",
      "(0, 0.38076062381574383)\n",
      "[0]\tval-auc:0.825384\ttrain-auc:0.826354\n",
      "[10]\tval-auc:0.830839\ttrain-auc:0.832889\n",
      "[20]\tval-auc:0.832485\ttrain-auc:0.835936\n",
      "[30]\tval-auc:0.833593\ttrain-auc:0.838538\n",
      "[40]\tval-auc:0.834494\ttrain-auc:0.840964\n",
      "[50]\tval-auc:0.835051\ttrain-auc:0.84305\n",
      "[60]\tval-auc:0.835401\ttrain-auc:0.844641\n",
      "[70]\tval-auc:0.835621\ttrain-auc:0.84608\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 0.923000097275 s \n",
      "\n",
      "(1, 0.38060784899837524)\n",
      "[0]\tval-auc:0.824985\ttrain-auc:0.826803\n",
      "[10]\tval-auc:0.830098\ttrain-auc:0.83317\n",
      "[20]\tval-auc:0.831767\ttrain-auc:0.836073\n",
      "[30]\tval-auc:0.832943\ttrain-auc:0.838753\n",
      "[40]\tval-auc:0.833904\ttrain-auc:0.841253\n",
      "[50]\tval-auc:0.834515\ttrain-auc:0.843287\n",
      "[60]\tval-auc:0.8349\ttrain-auc:0.845042\n",
      "[70]\tval-auc:0.835132\ttrain-auc:0.846386\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 0.960000038147 s \n",
      "\n",
      "(2, 0.37846186360928236)\n"
     ]
    }
   ],
   "source": [
    "df_cvfolds = []\n",
    "bst = []\n",
    "\n",
    "for fold in range(3):\n",
    "    train_subset = train[train.user_id % 3 != fold]\n",
    "    valid_subset = train[train.user_id % 3 == fold]\n",
    "\n",
    "    X_train = train_subset.drop('reordered', axis=1)\n",
    "    y_train = train_subset.reordered\n",
    "\n",
    "    X_val = valid_subset.drop('reordered', axis=1)\n",
    "    y_val = valid_subset.reordered\n",
    "\n",
    "    val_index = X_val[['user_id', 'product_id', 'order_id']]\n",
    "    \n",
    "    features_to_use = list(X_train.columns)\n",
    "    features_to_use.remove('user_id')\n",
    "    features_to_use.remove('product_id')\n",
    "    features_to_use.remove('order_id')\n",
    "\n",
    "    bst.append(xgboost_cv(X_train, y_train, X_val, y_val, features_to_use))\n",
    "    \n",
    "    d_test = xgboost.DMatrix(X_val[features_to_use], y_val) \n",
    "    \n",
    "    lim = .203\n",
    "    val_out = val_index.copy()\n",
    "\n",
    "    val_out.loc[:,'reordered'] = (bst[-1].predict(d_test) > lim).astype(int)\n",
    "    val_out.loc[:, 'product_id'] = val_out.product_id.astype(str)\n",
    "    presubmit = ka_add_groupby_features_n_vs_1(val_out[val_out.reordered == 1], \n",
    "                                                   group_columns_list=['order_id'],\n",
    "                                                   target_columns_list= ['product_id'],\n",
    "                                                   methods_list=[lambda x: ' '.join(set(x))], keep_only_stats=True)\n",
    "\n",
    "    presubmit = presubmit.set_index('order_id')\n",
    "    presubmit.columns = ['products']\n",
    "\n",
    "    fullfold = pd.DataFrame(index = val_out.order_id.unique())\n",
    "\n",
    "    fullfold.index.name = 'order_id'\n",
    "    fullfold['products'] = ['None'] * len(fullfold)\n",
    "\n",
    "    fullfold.loc[presubmit.index, 'products'] = presubmit.products\n",
    "\n",
    "    print(fold, compare_results(df_train_gt, fullfold))\n",
    "    \n",
    "    df_cvfolds.append(fullfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV的最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3799451371\n"
     ]
    }
   ],
   "source": [
    "df_cv = pd.concat(df_cvfolds)\n",
    "print(compare_results(df_train_gt, df_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部数据运行\n",
    "注释中是之前用的xgboost.cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(['user_id', 'product_id', 'order_id'], axis=1, inplace=True)\n",
    "y_train = train.reordered\n",
    "X_train = train.drop('reordered', axis = 1)\n",
    "#如果数据量太大无法带动 请用下面的X_train和y_train\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train.drop('reordered', axis=1), train.reordered,\n",
    "#                                                      test_size=0.2, random_state=42)\n",
    "\n",
    "d_train = xgboost.DMatrix(X_train, y_train)\n",
    "\n",
    "watchlist= [(d_train, \"train\")]\n",
    "#### tang\n",
    "# res = xgboost.cv(xgb_params, d_train, num_boost_round=10, nfold=3, seed=0,stratified=True,show_stdv=True)\n",
    "# cv_mean = res.iloc[-1, 0]  \n",
    "# cv_std = res.iloc[-1, 1]  \n",
    "# print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "# CV-Mean: 0.830923333333+0.000128857371625\n",
    "# CV-Mean: 0.830947+0.000159241954271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.827064\n",
      "[10]\ttrain-auc:0.832756\n",
      "[20]\ttrain-auc:0.835223\n",
      "[30]\ttrain-auc:0.837393\n",
      "[40]\ttrain-auc:0.839432\n",
      "[50]\ttrain-auc:0.841168\n",
      "[60]\ttrain-auc:0.842582\n",
      "[70]\ttrain-auc:0.843776\n",
      "[80]\ttrain-auc:0.844869\n",
      "[90]\ttrain-auc:0.845772\n",
      "[100]\ttrain-auc:0.846525\n",
      "[110]\ttrain-auc:0.847078\n",
      "[120]\ttrain-auc:0.847749\n",
      "[130]\ttrain-auc:0.84846\n",
      "[140]\ttrain-auc:0.849009\n",
      "[150]\ttrain-auc:0.849491\n",
      "[160]\ttrain-auc:0.849917\n",
      "[170]\ttrain-auc:0.850464\n"
     ]
    }
   ],
   "source": [
    "# xgb_params = {\n",
    "#     \"objective\"         : \"reg:logistic\"\n",
    "#     ,\"eval_metric\"      : \"auc\"\n",
    "#     ,\"eta\"              : 0.1\n",
    "#     ,\"max_depth\"        : 10\n",
    "#     ,\"min_child_weight\" : 5\n",
    "#     ,\"gamma\"            :0.70\n",
    "#     ,\"subsample\"        :0.95\n",
    "#     ,\"colsample_bytree\" :0.95\n",
    "#     ,\"alpha\"            :2e-05\n",
    "#     ,\"lambda\"           :10\n",
    "#     ,'tree_method'      :'exact'\n",
    "# }\n",
    "bst = xgboost.train(params=xgb_params, dtrain=d_train, num_boost_round=100, evals=watchlist, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清理一下内存 不然会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d3036921a36e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 1.4889998436 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_pickle('X_test.pkl')\n",
    "d_test = xgboost.DMatrix(X_test.drop(['eval_set', 'user_id', 'order_id', 'reordered', 'product_id'], axis=1))\n",
    "X_test.loc[:,'reordered'] = (bst.predict(d_test) > 0.203).astype(int)\n",
    "X_test.loc[:, 'product_id'] = X_test.product_id.astype(str)\n",
    "submit = ka_add_groupby_features_n_vs_1(X_test[X_test.reordered == 1], \n",
    "                                               group_columns_list=['order_id'],\n",
    "                                               target_columns_list= ['product_id'],\n",
    "                                               methods_list=[lambda x: ' '.join(set(x))], keep_only_stats=True)\n",
    "\n",
    "\n",
    "submit.columns = sample_submission.columns.tolist()\n",
    "submit_final = sample_submission[['order_id']].merge(submit, how='left').fillna('None')\n",
    "submit_final.to_csv(\"python_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果 0.3832204  RANK 14%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画feature_importance图\n",
    "需要解决怎么画个能看清楚的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xgboost.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
